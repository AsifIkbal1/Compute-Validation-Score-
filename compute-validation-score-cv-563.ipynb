{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Validation Notebook for My Candidate ReRank Model\nIn this notebook we compute validation score for my other notebook [here][10] which submits an LB 0.573 solution. To compute validation, we just need to load parquet files from a different Kaggle dataset. Instead of loading the real train and real test data. We load the first 3 week of original train as \"new train\". And the last 1 week of original train as \"new test\". Then we train our model with \"new train\" and predict \"new test\". Finally we compute competition metric from our predictions. The data and code for validation comes from Radek [here][11].\n\n# Notes\nBelow are notes about versions:\n* **Version 2 CV 0.5630** is validation for Candidate Rerank notebook version 1 with LB `0.573` \n* **Version 3 CV 0.5633** is validation for Candidate Rerank notebook version 2 with LB ???\n\n# Introduction from My Candidate ReRank Notebook\nIn this notebook, we present a \"candidate rerank\" model using handcrafted rules. We can improve this model by engineering features, merging them unto items and users, and training a reranker model (such as XGB) to choose our final 20. Furthermore to tune and improve this notebook, we should build a local CV scheme to experiment new logic and/or models.\n\nNote in this competition, a \"session\" actually means a unique \"user\". So our task is to predict what each of the `1,671,803` test \"users\" (i.e. \"sessions\") will do in the future. For each test \"user\" (i.e. \"session\") we must predict what they will `click`, `cart`, and `order` during the remainder of the week long test period.\n\n### Step 1 - Generate Candidates\nFor each test user, we generate possible choices, i.e. candidates. In this notebook, we generate candidates from 5 sources:\n* User history of clicks, carts, orders\n* Most popular 20 clicks, carts, orders during test week\n* Co-visitation matrix of click/cart/order to cart/order with type weighting\n* Co-visitation matrix of cart/order to cart/order called buy2buy\n* Co-visitation matrix of click/cart/order to clicks with time weighting\n\n### Step 2 - ReRank and Choose 20\nGiven the list of candidates, we must select 20 to be our predictions. In this notebook, we do this with a set of handcrafted rules. We can improve our predictions by training an XGBoost model to select for us. Our handcrafted rules give priority to:\n* Most recent previously visited items\n* Items previously visited multiple times\n* Items previously in cart or order\n* Co-visitation matrix of cart/order to cart/order\n* Current popular items\n\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Nov-2022/c_r_model.png)\n  \n# Credits\nWe thank many Kagglers who have shared ideas. We use co-visitation matrix idea from Vladimir [here][1]. We use groupby sort logic from Sinan in comment section [here][4]. We use duplicate prediction removal logic from Radek [here][5]. We use multiple visit logic from Pietro [here][2]. We use type weighting logic from Ingvaras [here][3]. We use leaky test data from my previous notebook [here][4]. And some ideas may have originated from Tawara [here][6] and KJ [here][7]. We use Colum2131's parquets [here][8]. Above image is from Ravi's discussion about candidate rerank models [here][9]\n\n[1]: https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix\n[2]: https://www.kaggle.com/code/pietromaldini1/multiple-clicks-vs-latest-items\n[3]: https://www.kaggle.com/code/ingvarasgalinskas/item-type-vs-multiple-clicks-vs-latest-items\n[4]: https://www.kaggle.com/code/cdeotte/test-data-leak-lb-boost\n[5]: https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic\n[6]: https://www.kaggle.com/code/ttahara/otto-mors-aid-frequency-baseline\n[7]: https://www.kaggle.com/code/whitelily/co-occurrence-baseline\n[8]: https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format\n[9]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364721\n[10]: https://www.kaggle.com/cdeotte/candidate-rerank-model-lb-0-573\n[11]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991","metadata":{"papermill":{"duration":0.005761,"end_time":"2022-11-10T16:03:24.627071","exception":false,"start_time":"2022-11-10T16:03:24.62131","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Step 1 - Candidate Generation with RAPIDS\nFor candidate generation, we build three co-visitation matrices. One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix. One computes the popularity of cart/order given a user's previous cart/order. We call this \"buy2buy\" matrix. One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix. We will use RAPIDS cuDF GPU to compute these matrices quickly!","metadata":{"papermill":{"duration":0.004043,"end_time":"2022-11-10T16:03:24.635669","exception":false,"start_time":"2022-11-10T16:03:24.631626","status":"completed"},"tags":[]}},{"cell_type":"code","source":"VER = 1\n\nimport pandas as pd, numpy as np\nfrom tqdm.notebook import tqdm\nimport os, sys, pickle, glob, gc\nfrom collections import Counter\nimport cudf, itertools\nprint('We will use RAPIDS version',cudf.__version__)","metadata":{"papermill":{"duration":2.845087,"end_time":"2022-11-10T16:03:27.484916","exception":false,"start_time":"2022-11-10T16:03:24.639829","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T17:54:24.383337Z","iopub.execute_input":"2022-11-10T17:54:24.383802Z","iopub.status.idle":"2022-11-10T17:54:27.384066Z","shell.execute_reply.started":"2022-11-10T17:54:24.383725Z","shell.execute_reply":"2022-11-10T17:54:27.381783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compute Three Co-visitation Matrices with RAPIDS\nWe will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 10x faster than using multi process CPU. Set the variable `DISK_PIECES` based on the GPU you are using to avoid memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix under 2 minutes! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes 9 minutes which is still faster than other notebooks using CPU which take 1 or 2 hours per co-visitation matrix.","metadata":{"papermill":{"duration":0.004309,"end_time":"2022-11-10T16:03:27.494144","exception":false,"start_time":"2022-11-10T16:03:27.489835","status":"completed"},"tags":[]}},{"cell_type":"code","source":"files = glob.glob('../input/otto-validation/*_parquet/*')\nCHUNK = int( np.ceil( len(files)/6 ))\nprint(f'We will process {len(files)} files in chunk size {CHUNK} files.')","metadata":{"papermill":{"duration":0.044165,"end_time":"2022-11-10T16:03:27.542811","exception":false,"start_time":"2022-11-10T16:03:27.498646","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T17:54:27.386158Z","iopub.execute_input":"2022-11-10T17:54:27.38681Z","iopub.status.idle":"2022-11-10T17:54:27.425224Z","shell.execute_reply.started":"2022-11-10T17:54:27.386773Z","shell.execute_reply":"2022-11-10T17:54:27.424293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted","metadata":{"papermill":{"duration":0.004349,"end_time":"2022-11-10T16:03:27.551761","exception":false,"start_time":"2022-11-10T16:03:27.547412","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ntype_labels = {'clicks':0, 'carts':1, 'orders':2}\ntype_weight = {0:1, 1:6, 2:3}\n\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b):\n            # READ FILE\n            df = cudf.read_parquet(files[k])\n            df.ts = (df.ts/1000).astype('int32')\n            df['type'] = df['type'].map(type_labels).astype('int8')\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = df.type_y.map(type_weight)\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<40].drop('n',axis=1)\n    # SAVE PART TO DISK\n    df = tmp.to_pandas().groupby('aid_x').aid_y.apply(list)\n    with open(f'top_40_carts_orders_v{VER}_{PART}.pkl', 'wb') as f:\n        pickle.dump(df.to_dict(), f)","metadata":{"papermill":{"duration":403.713782,"end_time":"2022-11-10T16:10:11.270161","exception":false,"start_time":"2022-11-10T16:03:27.556379","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T17:54:27.42643Z","iopub.execute_input":"2022-11-10T17:54:27.42678Z","iopub.status.idle":"2022-11-10T18:00:59.522528Z","shell.execute_reply.started":"2022-11-10T17:54:27.426747Z","shell.execute_reply":"2022-11-10T18:00:59.521445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2) \"Buy2Buy\" Co-visitation Matrix","metadata":{"papermill":{"duration":0.030783,"end_time":"2022-11-10T16:10:11.331839","exception":false,"start_time":"2022-11-10T16:10:11.301056","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ntype_labels = {'clicks':0, 'carts':1, 'orders':2}\n\nDISK_PIECES = 1\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b):\n            # READ FILE\n            df = cudf.read_parquet(files[k])\n            df.ts = (df.ts/1000).astype('int32')\n            df['type'] = df['type'].map(type_labels).astype('int8')\n            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = 1\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<40].drop('n',axis=1)\n    # SAVE PART TO DISK\n    df = tmp.to_pandas().groupby('aid_x').aid_y.apply(list)\n    with open(f'top_40_buy2buy_v{VER}_{PART}.pkl', 'wb') as f:\n        pickle.dump(df.to_dict(), f)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":84.918311,"end_time":"2022-11-10T16:11:36.280983","exception":false,"start_time":"2022-11-10T16:10:11.362672","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T18:00:59.525706Z","iopub.execute_input":"2022-11-10T18:00:59.526281Z","iopub.status.idle":"2022-11-10T18:02:20.91658Z","shell.execute_reply.started":"2022-11-10T18:00:59.526251Z","shell.execute_reply":"2022-11-10T18:02:20.915489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3) \"Clicks\" Co-visitation Matrix - Time Weighted","metadata":{"papermill":{"duration":0.036979,"end_time":"2022-11-10T16:11:36.356076","exception":false,"start_time":"2022-11-10T16:11:36.319097","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ntype_labels = {'clicks':0, 'carts':1, 'orders':2}\n\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b):\n            # READ FILE\n            df = cudf.read_parquet(files[k])\n            df.ts = (df.ts/1000).astype('int32')\n            df['type'] = df['type'].map(type_labels).astype('int8')\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<40].drop('n',axis=1)\n    # SAVE PART TO DISK\n    df = tmp.to_pandas().groupby('aid_x').aid_y.apply(list)\n    with open(f'top_40_clicks_v{VER}_{PART}.pkl', 'wb') as f:\n        pickle.dump(df.to_dict(), f)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2022-11-10T16:11:36.393657","status":"running"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T18:02:20.918522Z","iopub.execute_input":"2022-11-10T18:02:20.918995Z","iopub.status.idle":"2022-11-10T18:08:29.218997Z","shell.execute_reply.started":"2022-11-10T18:02:20.918956Z","shell.execute_reply":"2022-11-10T18:08:29.217912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2 - ReRank (choose 20) using handcrafted rules\nFor description of the handcrafted rules, read this notebook's intro.","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"def load_test():    \n    dfs = []\n    for e, chunk_file in enumerate(glob.glob('../input/otto-validation/test_parquet/*')):\n        chunk = pd.read_parquet(chunk_file)\n        dfs.append(chunk)\n    return pd.concat(dfs).reset_index(drop=True).astype({\"ts\": \"datetime64[ms]\"})\n\ntest_df = load_test()\nprint('Test data has shape',test_df.shape)\ntest_df.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T18:08:29.221246Z","iopub.execute_input":"2022-11-10T18:08:29.222092Z","iopub.status.idle":"2022-11-10T18:08:31.210139Z","shell.execute_reply.started":"2022-11-10T18:08:29.222054Z","shell.execute_reply":"2022-11-10T18:08:31.209078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# THREE CO-VISITATION MATRICES\ntop_20_clicks = pickle.load(open(f'top_40_clicks_v{VER}_0.pkl', 'rb'))\nfor k in range(1,DISK_PIECES): \n    top_20_clicks.update( pickle.load(open(f'top_40_clicks_v{VER}_{k}.pkl', 'rb')) )\ntop_20_buys = pickle.load(open(f'top_40_carts_orders_v{VER}_0.pkl', 'rb'))\nfor k in range(1,DISK_PIECES): \n    top_20_buys.update( pickle.load(open(f'top_40_carts_orders_v{VER}_{k}.pkl', 'rb')) )\ntop_20_buy2buy = pickle.load(open(f'top_40_buy2buy_v{VER}_0.pkl', 'rb'))\n\n# TOP CLICKS AND ORDERS IN TEST\ntop_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:20]\ntop_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:20]\n\nprint('Here are size of our 3 co-visitation matrices:')\nlen( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys )","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T18:08:31.21169Z","iopub.execute_input":"2022-11-10T18:08:31.212626Z","iopub.status.idle":"2022-11-10T18:08:55.706728Z","shell.execute_reply.started":"2022-11-10T18:08:31.212586Z","shell.execute_reply":"2022-11-10T18:08:55.705712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type_weight_multipliers = {'clicks': 1, 'carts': 6, 'orders': 3}\n\ndef suggest_clicks(df):\n    # USE USER HISTORY AIDS AND TYPES\n    aids=df.aid.tolist()\n    types = df.type.tolist()\n    unique_aids = list(dict.fromkeys(aids[::-1] ))\n    # RERANK CANDIDATES USING WEIGHTS\n    if len(unique_aids)>=20:\n        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n        aids_temp = Counter() \n        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n        for aid,w,t in zip(aids,weights,types): \n            aids_temp[aid] += w * type_weight_multipliers[t]\n        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n        return sorted_aids\n    # USE \"CLICKS\" CO-VISITATION MATRIX\n    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n    # RERANK CANDIDATES\n    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n    # USE TOP20 TEST CLICKS\n    return result + list(top_clicks)[:20-len(result)]\n\ndef suggest_buys(df):\n    # USE USER HISTORY AIDS AND TYPES\n    aids=df.aid.tolist()\n    types = df.type.tolist()\n    # UNIQUE AIDS AND UNIQUE BUYS\n    unique_aids = list(dict.fromkeys(aids[::-1] ))\n    df = df.loc[(df['type']=='carts')|(df['type']=='orders')]\n    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n    # RERANK CANDIDATES USING WEIGHTS\n    if len(unique_aids)>=20:\n        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n        aids_temp = Counter() \n        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n        for aid,w,t in zip(aids,weights,types): \n            aids_temp[aid] += w * type_weight_multipliers[t]\n        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n        for aid in aids3: aids_temp[aid] += 0.1\n        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n        return sorted_aids\n    # USE \"CART ORDER\" CO-VISITATION MATRIX\n    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n    # RERANK CANDIDATES\n    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids] \n    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n    # USE TOP20 TEST ORDERS\n    return result + list(top_orders)[:20-len(result)]","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T18:08:55.708514Z","iopub.execute_input":"2022-11-10T18:08:55.709244Z","iopub.status.idle":"2022-11-10T18:08:55.725595Z","shell.execute_reply.started":"2022-11-10T18:08:55.709204Z","shell.execute_reply":"2022-11-10T18:08:55.724573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission CSV\nInferring test data with Pandas groupby is slow. We need to accelerate the following code.","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"%%time\npred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n    lambda x: suggest_clicks(x)\n)\n\npred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n    lambda x: suggest_buys(x)\n)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T18:08:55.726817Z","iopub.execute_input":"2022-11-10T18:08:55.727778Z","iopub.status.idle":"2022-11-10T18:45:38.520156Z","shell.execute_reply.started":"2022-11-10T18:08:55.727743Z","shell.execute_reply":"2022-11-10T18:45:38.51909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\norders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\ncarts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T18:45:38.523508Z","iopub.execute_input":"2022-11-10T18:45:38.523794Z","iopub.status.idle":"2022-11-10T18:45:42.053061Z","shell.execute_reply.started":"2022-11-10T18:45:38.523767Z","shell.execute_reply":"2022-11-10T18:45:42.05206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\npred_df.columns = [\"session_type\", \"labels\"]\npred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\npred_df.to_csv(\"validation_preds.csv\", index=False)\npred_df.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T18:45:42.054632Z","iopub.execute_input":"2022-11-10T18:45:42.055Z","iopub.status.idle":"2022-11-10T18:46:26.674641Z","shell.execute_reply.started":"2022-11-10T18:45:42.054968Z","shell.execute_reply":"2022-11-10T18:46:26.673371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute Validation Score\nThis code is from Radek [here][1]. It has been modified to use less memory.\n\n[1]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# FREE MEMORY\ndel pred_df_clicks, pred_df_buys, clicks_pred_df, orders_pred_df, carts_pred_df, tmp\ndel top_20_clicks, top_20_buy2buy, top_20_buys, top_clicks, top_orders, test_df, df\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:49:41.011964Z","iopub.execute_input":"2022-11-10T18:49:41.01238Z","iopub.status.idle":"2022-11-10T18:49:57.703109Z","shell.execute_reply.started":"2022-11-10T18:49:41.012349Z","shell.execute_reply":"2022-11-10T18:49:57.701759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# COMPUTE METRIC\nscore = 0\nweights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\nfor t in ['clicks','carts','orders']:\n    sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n    sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n    test_labels = pd.read_parquet('../input/otto-validation/test_labels.parquet')\n    test_labels = test_labels.loc[test_labels['type']==t]\n    test_labels = test_labels.merge(sub, how='left', on=['session'])\n    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n    score += weights[t]*recall\n    print(f'{t} recall =',recall)\n    \nprint('=============')\nprint('Overall Recall =',score)\nprint('=============')","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-11-10T18:50:21.882951Z","iopub.execute_input":"2022-11-10T18:50:21.883347Z","iopub.status.idle":"2022-11-10T18:52:15.063085Z","shell.execute_reply.started":"2022-11-10T18:50:21.883314Z","shell.execute_reply":"2022-11-10T18:52:15.061927Z"},"trusted":true},"execution_count":null,"outputs":[]}]}